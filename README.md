# Comparison of software vulnerability detector

System & Software Security 2021, 2nd assignment

> by AndrÃ¡s Schmelczer, Joana Trashlieva, and Leonardo Pohl

## Dataset

[Juliet Test Suite for C#](https://samate.nist.gov/SRD/testsuites/juliet/Juliet_Test_Suite_v1.3_for_C%23.zip)

> Aug 2020, NSA Center for Assured Software, 28942 cases

### Paths

- [Original source](Juliet_Test_Suite_v1.3_for_C#/src)
- [Only correct cases](Juliet_Test_Suite_v1.3_for_C#/src-omit-bad)
- [Only wrong cases](Juliet_Test_Suite_v1.3_for_C#/src-omit-good)
  > The last two source code versions have the defines physically removed. This was necessary for the CodeQL compilation pipeline, and SonarQube.

This were generated by the `augment_data.py` script.

## Benchmarking

### CodeQL with LGTM rules (csharp-lgtm-full.qls)

1. `cd Juliet_Test_Suite_v1.3_for_C#`
2. `python3 run_analysis_codeql.py`

   > Builds and analyses every project, and outputs a `results.csv` file inside the `src-omit-good` and `src-omit-bad` directories' projects.

   > _The `results.csv` files were already generated and added to version control._

3. `python3 gather_results_codeql.py > ../code-ql-results.log`

   > Merges the results of every `results.csv` file.

   > _The results were already added to version control._

#### CodeQL automated GitHub integration

Since GitHUB acquiring LGTM, a streamlined integration is available to create a GitHub action executing some CodeQL queries. This can be seen in [codeql-analysis.yml](.github/workflows/codeql-analysis.yml).

### SonarQube

### SNYK

## Results

### With only errors included

|                         | CodeQL | SonarQube | SNYK |
| :---------------------- | -----: | --------: | ---: |
| Precision (with errors) |   0.85 |           |      |
| Recall (with errors)    |   0.04 |           |      |
| F1 (with errors)        |   0.07 |           |      |

### With every finding included

|                           | CodeQL | SonarQube | SNYK |
| :------------------------ | -----: | --------: | ---: |
| Precision (with warnings) |   0.50 |           |      |
| Recall (with warnings)    |   0.43 |           |      |
| F1 (with warnings)        |   0.46 |           |      |

###

###
